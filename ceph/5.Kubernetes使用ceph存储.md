## 背景介绍

使用公有云的Kubernetes集群的时候, 存储一般使用公有云提供的, 在私有云上就需要自己提供了, 业界使用的比较多的就是ceph了. 这里介绍一下如何在Kubernetes集群中使用ceph集群. 技术一直在迭代, 未来不一定能用到, 只是记录一下曾经的经历.



ceph支持rbd块存储/cephFS文件系统/对象存储, 这里主要说的是rbd块存储.

## 具体配置

>  操作之前先搭建好Kubernetes集群和ceph集群.

#### 创建账号

这里直接复制官方文档中的创建语句, 然后根据自己的情况修改即可. ceph的语句比较不好理解, 也就不用去刻意理解了, 反正按照这个创建完的账号是可以正常读写的, 一般也不用去动.

```shell
# client是固定格式, kubernetes是账号, 只需要按照实际替换pool的名字即可, 就会生成下面的账号密码

$ ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=test01' mgr 'profile rbd pool=test01'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
```

#### 生成Kubernetes连接ceph配置文件

1. 查询fsid和Monitor地址

```
[root@server131 ~]# ceph mon dump
epoch 3
fsid f9833c52-c140-11f0-901c-000c29434f9e
last_changed 2025-11-17T08:13:16.907281+0000
created 2025-11-14T10:03:07.582770+0000
min_mon_release 16 (pacific)
election_strategy: 1
0: [v2:192.168.78.131:3300/0,v1:192.168.78.131:6789/0] mon.server131
1: [v2:192.168.78.132:3300/0,v1:192.168.78.132:6789/0] mon.server132
2: [v2:192.168.78.133:3300/0,v1:192.168.78.133:6789/0] mon.server133
dumped monmap epoch 3
```



2. 生成configmap

将上面查询到的fsid和Monitor地址填入下面的文件

```
$ cat  csi-config-map.yaml

apiVersion: v1
kind: Namespace
metadata:
  name: ceph


---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  namespace: ceph
  name: ceph-csi-config

$ kubectl apply -f csi-config-map.yaml
```



3. 配置密钥

```
$ cat csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: ceph
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==

$ kubectl apply -f csi-rbd-secret.yaml
```

4. 一些额外的配置

一些关于安全密钥之类的配置, 按照文档执行就可以.

```
$ cat  csi-kms-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    {}
metadata:
  namespace: ceph
  name: ceph-csi-encryption-kms-config
  
$ kubectl apply -f csi-kms-config-map.yaml

$ cat ceph-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  ceph.conf: |
    [global]
    auth_cluster_required = cephx
    auth_service_required = cephx
    auth_client_required = cephx
  # keyring is a required key and its value should be empty
  keyring: |
metadata:
  namespace: ceph
  name: ceph-config
  
$ kubectl apply -f ceph-config-map.yaml

```



#### 部署CSI驱动

```
$ wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
$ wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml
$ wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
$ wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml

安装的时候都会统一放到ceph的namespace下面, 默认的是default, 所以需要下载下来修改一下, 然后执行, 同时也要参考https://github.com/ceph/ceph-csi/, 使用对应的版本, 默认的yaml里面配置的是开发版本.

$ kubectl apply -f csi-provisioner-rbac.yaml
$ kubectl apply -f csi-nodeplugin-rbac.yaml
$ kubectl apply -f csi-rbdplugin-provisioner.yaml
$ kubectl apply -f csi-rbdplugin.yaml
```



#### 创建StorageClass

这里可以通过创建不同类型的pool来实现不同的使用场景, 比如SSD, HDD等.

> 注意, 这里的parameters那块一定要有expand参数, 不然以后是没办法扩容的, 这个是一个坑, 旧的官方文档没有, 新的官方文档才有.

```
$ cat  csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   imageFeatures: layering
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: ceph
   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
   csi.storage.k8s.io/controller-expand-secret-namespace: ceph
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: ceph
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
   - discard

$ kubectl apply -f csi-rbd-sc.yaml
```



#### 创建PVC

```
$ cat test-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc

$ kubectl apply -f test-pvc.yaml
```



#### 创建Deployment



```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-app
  name: test-app
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-app
  template:
    metadata:
      labels:
        app: test-app
    spec:
      containers:
      - name: test-app
        image: rockylinux:9
        imagePullPolicy: IfNotPresent
        command: ["python3", "-m", "http.server", "8888"]
        volumeMounts:
        - mountPath: /data
          name: data

      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: test-pvc
```





到此为止搞定!



## 参考文档



* https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/

  