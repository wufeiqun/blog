## 背景介绍

之前一直在使用云产品, 包括服务器/云盘等等, 自己对于自建机房的技术栈不是很了解, 正好目前的公司使用的是Ceph分布式存储, 所以有机会去深入学习了一下, 这里记录一下安装的过程.



## Ceph简介

Ceph可以提供块存储(RBD), 对象存储(RADOSGW兼容S3), 文件系统存储(CEPHFS)等, 高可用, 可扩展, 成熟. 

<img width="700" height="297" alt="Image" src="https://github.com/user-attachments/assets/37a37cee-ad54-4722-8578-837b10a65a92" />

<img width="772" height="468" alt="Image" src="https://github.com/user-attachments/assets/f4865762-1cb5-4721-b2a1-18cd30d6ac49" />

**一句话总结就是: 一站式企业级存储解决方案!**



## 服务器信息

|  主机名   |     IP地址     |
| :-------: | :------------: |
| server131 | 192.168.78.131 |
| server132 | 192.168.78.132 |
| server133 | 192.168.78.133 |



操作系统选择RockyLinux9.6, 因为只是为了验证功能, 所以只使用了3个虚拟机, 并且monitor和OSD都部署在一起了, 真正生产使用的话monitor一般会跟OSD区分开.

## 版本选择



一般生产建议选择次新版, 并且一定是LTS版本.  我在本地使用RockyLinux9.6测试的时候, 安装新版本添加OSD总是报错, 咨询了ChatGPT也没有解决, 只能使用pacific(16)版本的才没问题, 这次我就使用16.2.15版本来做一个示例.

## 组件介绍

|  组件   | 是否必选 |                             作用                             |
| :-----: | :------: | :----------------------------------------------------------: |
| Monitor |   必选   | 通过Paxos协议组成集群对外提供高可用地址, 类似于Redis的Sentinel模式, 一般3个节点即可 |
|   OSD   |   必选   | 每一个数据盘会启动一个OSD进程, 用于处理跟磁盘的交互, 是Ceph最核心的组件 |
| Manager |   必选   | 提供Dashboard/监控等一些管理的能力, 一般2个节点就可以, 一主一备 |
|   MDS   |  非必选  | CephFS服务依赖的元数据服务。负责保存文件系统的元数据，管理目录结构, 如果不使用CephFS可以不安装 |
|   RGW   |  非必选  | 提供兼容S3的Restful API, 如果不需要对象存储功能可以不用安装  |

## 部署方式

官方推荐的部署方式为Cephadm, Cephadm依赖Python3和Docker(或者podman).



## 服务器配置

#### 修改主机名

这一步很关键, 因为Ceph的crush算法会把主机名当做一个因子, 为了把多副本的数据尽量打散, 所有主机名都一样的话会导致Ceph把多副本的数据都集中放到某一个服务器上的OSD里面, 这样该服务器出问题的时候就会影响服务.

```
# hostnamectl set-hostname server132
# hostnamectl set-hostname --static server132
```





## 部署过程























