#### 背景介绍

&emsp;接触`Prometheus`已经有半年了, 之前因为工作忙, 一直帮助开发做一些体力活, 配置数据源等,工作, 从来没有理清楚`Prometheus`的整个过程, 对这个监控系统也是一知半解, 最近终于有了时间自己写接口了, 正好趁这个机会给自己的接口服务加上了监控.这里总结一下整个过程.

#### Prometheus agent部分

&emsp;这次加监控的接口是采用flask编写的一个部署回滚服务, 说白了提供两个接口, 一个部署, 一个回滚, 部分代码如下:

```python
@app.route("/deploy", methods=["POST"])
def deploy():
    if request.method == "POST":
        data = request.get_json()
        return json.dumps({"status_code": 200, "data": "部署成功"})


@app.route("/rollback", methods=["POST"])
def rollback():
    if request.method == "POST":
        data = request.get_json()
        return json.dumps({"status_code": 200, "data": "回滚成功!"})
```

服务采用的`gunicorn`来启动的, 开始只是简单地参考了[prometheus_client](https://github.com/prometheus/client_python)文档, 向代码中加入了接口请求次数的计数器, 如下:

```python
REQUEST_COUNTER = Counter("request_processing_counter", "接口调用计数器")

@app.route("/deploy", methods=["POST"])
def deploy():
    if request.method == "POST":
        REQUEST_COUNTER.inc()
        data = request.get_json()
        return json.dumps({"status_code": 200, "data": "部署成功"})
```

> 注意: `request_processing_counter`这个命名中不能包含`.`

模拟请求了几次, 访问统计页面的时候发现, 有时候统计的数正确, 有时候统计的数据为0, 这就尴尬了, 为啥还有不一样的呢, 后来仔细想了一下, 我的服务使用的是`Gunicorn`启动的, 这是多进程的模型, 而统计的代码分布在不同的进程中, 不同的进程并没有共享统计的结果数据, 所以请求统计页面的时候有时候有数据, 有时候没数据, 后来看了一下官方的文档, 居然支持gunicorn这种启动方式的采集, 这里具体说一下:

* 定义`prometheus_multiproc_dir`系统变量

`prometheus_client`将每个gunicorn worker的统计数据会放到这个系统变量定义的目录中, 所以在启动程序前一定要定义这个系统变量, 我是用的是supervisor启动的, 定义方式如下:

```
[program:deploy]
command = /usr/local/python3/bin/gunicorn -c conf/gunicorn.py manage:app
environment=prometheus_multiproc_dir='/root/tmp'
directory = /root/deploy
autostart = true
autorestart = true
stdout_logfile=/root/logs/deploy_stdout.log
stdout_logfile_maxbytes=50MB
stdout_logfile_backups=1
stderr_logfile=/root/logs/deploy_stderr.log
stderr_logfile_maxbytes=50MB
stderr_logfile_backups=1
``` 

另一个要注意的点是, 每次重启服务的时候最好清空一下这个目录, 不然历史的统计数据会依然存在.

* 将如下代码贴入你的`gunicorn.py`配置文件中:

```python
def worker_exit(server, worker):
    from prometheus_client import multiprocess
    multiprocess.mark_process_dead(worker.pid)
```

* 代码逻辑部分示例

Counters, Summarys and Histograms 跟之前用法一致. Gauge有点不同, 由于这里暂时没有用到, 没有去了解.

```
import os
from flask import Flask, Response
from prometheus_client import multiprocess
from prometheus_client import generate_latest, CollectorRegistry, CONTENT_TYPE_LATEST, Gauge, Counter

app = Flask(__name__)

NUM_REQUESTS = Counter("num_requests", "Example counter")


@app.route("/")
def hello():
    NUM_REQUESTS.inc()
    return "Hello World from {}!".format(os.getpid())


@app.route("/metrics")
def metrics():
    registry = CollectorRegistry()
    multiprocess.MultiProcessCollector(registry)
    data = generate_latest(registry)
    return Response(data, mimetype=CONTENT_TYPE_LATEST)
```

到这里算是配置完毕了, 使用`Prometheus`采集`Gunicorn+Flask`的数据算是解决了.

那么问题来了, 我们要查看接口的QPS的话, agent需要上报什么数据呢?其实很简单, 只需要在程序里面定义一个计数器, 在接口处, 每次进来一个请求以后加1即可.为了更加合理的使用`prometheus`, 我们在程序中定义一个`Counter`计数器变量, 不同的接口根据不同的`label`来区分不同的数据. 代码如下:

```python

REQUEST_COUNTER = Counter("request_processing_counter", "接口调用计数器",  ["interface"])


@app.route("/deploy", methods=["POST"])
def deploy():
    if request.method == "POST":
        REQUEST_COUNTER.labels(interface="/deploy").inc()
        return json.dumps({"status_code": 200, "data": "部署成功"})

@app.route("/rollback", methods=["POST"])
def rollback():
    if request.method == "POST":
        REQUEST_COUNTER.labels(interface="/rollback").inc()
```

上面的代码, 部署以后, 从服务器本机查看得到的数据如下:

```
[root@mp-sre-deploy tmp]# curl http://127.0.0.1:80/metrics
# HELP request_processing_counter Multiprocess metric
# TYPE request_processing_counter counter
request_processing_counter{interface="/deploy"} 21.0
request_processing_counter{interface="/rollback"} 36.0
``` 

经过`prometheus`服务端抓取以后的数据如下, 这块暂时不具体说抓取数据这块, 后续会专门讲解的.

![image](https://user-images.githubusercontent.com/7486508/41807780-96473b34-7706-11e8-9c12-777d84b3fb34.png)

从上图可以看出来, 经过Prometheus服务端抓取以后, 所有的数据多了2个label, 一个是`instace="192.168.1.1:80"`, 另一个是`job="mp-sre-deploy"`, 第一个就是抓取的那个地址和端口, 第二个是抓取那块配置的, 所以我们在代码那边不用配置机器的信息, 抓取的时候会自动添加上.到目前为止, 我们得到了如上的数据, 下面开始使用, grafna画图.

#### 使用grafna画图

画图之前我们需要添加数据源, 这块有几个点要注意:

* 数据源的名称一定要跟项目关联起来, 不能随便起, 比如我的项目是`mp-sre-deploy`, 那么添加数据源的时候我可以添加两个, 基础监控的叫做`mp-sre-deploy-base`, 服务监控的数据源叫做`mp-sre-deploy`, 这样画图的时候就容易选择数据源了
* 注意数据源的类型和数据源跟grafna主机之间的安全组规则.

比如说我想要看到的`QPS`的图的效果如下:

![image](https://user-images.githubusercontent.com/7486508/41807913-b074227c-7708-11e8-90c1-c41f84a41387.png)

可以从上面选择不同主机和接口来查看具体的图表, 这是后就用到了grafna的一些基本知识, grafna原生支持prometheus查询语句, 并且支持变量, 这里就是用到了变量的概念.设置变量的方法如下:

下面的前提是先添加好数据源和新建两个dashboard, 一个基础的一个业务的, 基础的可以通过模板导入, 业务的可以自己画图.

![image](https://user-images.githubusercontent.com/7486508/41807948-380061ba-7709-11e8-9ea6-2a84e6716e01.png)

![image](https://user-images.githubusercontent.com/7486508/41808288-c910efb8-770d-11e8-8c03-ec66ba1d5776.png)

![image](https://user-images.githubusercontent.com/7486508/41808321-2683cbd4-770e-11e8-813b-8fdbc8050ee0.png)

接口QPS画图的公式为:

```
rate(request_processing_counter{interface="$api", instance="$instance"}[1m])
```

这里有2个点要注意:

* 注意变量的引用方式, 是在引号中的.
* 查询语句中的1m代表1分钟内的QPS, 查询的范围一定要大于抓取的粒度, 一般2倍,Prometheus抓取那边配置的时间粒度为30s, 这里配置的是1m.

到这里以后所有按照上面你的需求已经画出了图.

![image](https://user-images.githubusercontent.com/7486508/41808494-c2501610-7710-11e8-87fc-dd4bb919561f.png)

到这里QPS算是全部搞完了, 下一步开始学习接口响应时间的计算.
