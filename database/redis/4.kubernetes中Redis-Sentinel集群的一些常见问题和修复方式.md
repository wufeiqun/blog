# 背景介绍

公司生产环境Redis采用Sentinel集群的方式部署在Kubernetes里面. 自从迁移到PVE以后, 经常出现主节点漂移的现象, 明显比之前采用VMware的时候更加频繁. 一般来说, 在虚拟机上部署的Redis集群主节点漂移其实是不影响业务使用的, 因为每个节点的IP都是不变的, 不管哪个是主节点哪个是从节点每个节点都可以正常可用的, 客户端都可以正常连接使用.

但是部署在Kubernetes中情况就比较复杂了,  每次发生主从漂移, 大概率是因为某一个POD重启导致的, 当然也有因为单纯因为网络超时导致的. 如果是因为POD重启导致的这种情况的话, 每次重启POD都会换新的IP, 这是Kubernetes的机制, Sentinel集群默认不会删除历史用过的IP记录, 这是Sentinel的机制, 这两个机制在一起就会出问题.

部署在Kubernetes里面的Redis集群如果经过多次重启, 就会留下多个垃圾的IP, 这些垃圾IP会给到客户端使用, 会影响业务, 常见的报错如下:

```Plain
at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[app.jar:1.0-SNAPSHOT]
        at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[app.jar:1.0-SNAPSHOT]
        at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[app.jar:1.0-SNAPSHOT]
Caused by: org.redisson.client.RedisConnectionException: SlaveConnectionPool no available Redis entries. Master entry host: 10.80.2.213/10.80.2.213:6379 Disconnected hosts: [10.80.32.148/10.80.32.148:6379, 10.80.21.59/10.80.21.59:6379, 10.80.18.247/10.80.18.247:6379, 10.80.2.213/10.80.2.213:6379]
        at org.redisson.connection.pool.ConnectionPool.get(ConnectionPool.java:187) ~[redisson-3.23.1.jar!/:3.23.1]
        at org.redisson.connection.pool.SlaveConnectionPool.get(SlaveConnectionPool.java:30) ~[redisson-3.23.1.jar!/:3.23.1]
        at org.redisson.connection.balancer.LoadBalancerManager.nextConnection(LoadBalancerManager.java:321) ~[redisson-3.23.1.jar!/:3.23.1]
        at org.redisson.connection.MasterSlaveEntry.connectionReadOp(MasterSlaveEntry.java:597) ~[redisson-3.23.1.jar!/:3.23.1]
        at org.redisson.command.RedisExecutor.connectionReadOp(RedisExecutor.java:746) ~[redisson-3.23.1.jar!/:3.23.1]
        at org.redisson.command.RedisExecutor.getConnection(RedisExecutor.java:670) ~[redisson-3.23.1.jar!/:3.23.1]
        at org.redisson.command.RedisExecutor.execute(RedisExecutor.java:128) ~[redisson-3.23.1.jar!/:3.23.1]
        at org.redisson.command.RedisExecutor$1.run(RedisExecutor.java:317) ~[redisson-3.23.1.jar!/:3.23.1]
        at io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(Hash
```

偶尔某一个Redis节点也会出现隔离的现象, 导致只能查询一个从节点, 这时候会出现如下的报错, 这个跟上面的不一样, 但是解决方式类似, 所以放到这里一起总结:

```
RedisConnectionException: 
SENTINEL SENTINELS command returns less than 2 nodes!
At least two sentinels should be defined in Redis configuration.
Set checkSentinelsList = false to avoid this check.
```





# 解决方式

## 重置从节点

依次登录每一个Redis节点执行如下命令:

> 注意: 执行命令后10秒钟后执行, 等待执行完后再执行其它的节点, 每个节点都需要执行

```Plain
redis-cli -p 26379 sentinel reset * 
```

## 切换主从

切换主从本来跟上面的重置从节点没有啥关系, 两者没有必然联系, 但是目前没有更好监控脏IP的方式, 出现切换主从也一般会出现; 暂时可以通过这个方式来作为上面问题的一个监控方式.

```Plain
# 在任意节点执行即可, 目标是让0节点成为master节点
redis-cli -p 26379 SENTINEL failover mymaster
```

## 检查当前master

```Plain
redis-cli -p 26379 SENTINEL master mymaster
redis-cli -p 26379 SENTINEL slaves mymaster
```





## 官方文档参考

```
Removing the old master or unreachable replicas
移除旧主或无法访问的副本 
Sentinels never forget about replicas of a given master, even when they are unreachable for a long time. This is useful, because Sentinels should be able to correctly reconfigure a returning replica after a network partition or a failure event.
哨兵永远不会忘记某位主人的复制品，即使他们长时间无法触及。这很有用，因为哨兵应能在网络分区或故障事件后正确重配置返回副本。

Moreover, after a failover, the failed over master is virtually added as a replica of the new master, this way it will be reconfigured to replicate with the new master as soon as it will be available again.
此外，故障切换后，失败的主节点会被虚拟地添加为新主节点的副本，这样一旦新主机再次可用，它就会被重新配置为与新主机复制。

However sometimes you want to remove a replica (that may be the old master) forever from the list of replicas monitored by Sentinels.
不过有时候你想永远从哨兵监控的副本列表中移除一个副本（可能是旧主备份）。

In order to do this, you need to send a SENTINEL RESET mastername command to all the Sentinels: they'll refresh the list of replicas within the next 10 seconds, only adding the ones listed as correctly replicating from the current master INFO output.
为此，你需要向所有哨兵发送 SENTINEL RESET <mastername>：它们会在接下来的 10 秒内刷新副本列表，只添加当前主 INFO 输出中显示正确复制的副本。
```

* https://redis.io/docs/latest/operate/oss_and_stack/management/sentinel/



