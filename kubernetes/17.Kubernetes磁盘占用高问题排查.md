## 背景介绍

对于Kubernetes集群来说, worker节点的磁盘使用率是一个经常出问题的点, 所以我这里总结了一下常见的排查思路.


## 排查过程

#### 确认分区
使用`lsblk`命令确认下分区情况:
```
[root@k8s-worker-01 ~]# lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0              11:0    1 1024M  0 rom  
fd0               2:0    1    4K  0 disk 
sda               8:0    0  100G  0 disk 
├─sda2            8:2    0   99G  0 part 
│ ├─centos-swap 253:1    0    2G  0 lvm  
│ └─centos-root 253:0    0   97G  0 lvm  /
└─sda1            8:1    0    1G  0 part /boot
```

#### 按照占用空间排序

一般worker的磁盘占用比较多的目录有以下3个:

* /var/lib/docker
* /var/lib/containerd
* /var/lib/kubelet
* /var/log (可以单独关注)

可以先查看下`/var/lib`目录的磁盘占用, 并按照大小排序:

```
du -h --max-depth=1 /var/lib | sort -hr | head -10
```

通过上面就可以知道哪几个目录占用的磁盘空间大了,  然后再继续排查占用大的次级目录.

#### Docker目录排查

如果使用的是docker作为运行时, 会查询如下情况:

```
# du -h --max-depth=1 /var/lib/docker/overlay2 | sort -hr | head -10
53G     /var/lib/docker/overlay2
2.0G    /var/lib/docker/overlay2/676108c157ad36f6930bcd9de0eeee6b55d1f296a66203c076eb233f7de8d3f4
1.2G    /var/lib/docker/overlay2/ce6f67378a4130776f87559881109436d4fed7819eaccde75e1047fc37087319
```


```
# 输出依次为，进程pid、容器ID、容器名、存储work路径，即可确定是哪个容器。 可以结合grep
docker ps -q|xargs docker inspect --format '{{.State.Pid}}, {{.Id}}, {{.Name}}, {{.GraphDriver.Data.WorkDir}}' | grep 上述ID串

40647, 68b6a1f6f13bd10bf975c29333fcc397a0468ec8169dae6fdaafebbb31f9ba78, /gitlab-runner, /var/lib/docker/overlay2/9dc802e224f7c43ff37216340fe1f39a86dc69834b7223037ed7dfd32119b53b/work
47934, c39f7931ce4ba541b595a0ce4b42174cd4968e9a5307650f4cbcf12d68edfed1, /kafka_kafka_1, /var/lib/docker/overlay2/6ae3e5f0bd6b21e656bec75663fa43c0fb478e560d6905bf20df140479009e21/work
```



#### kubelet目录排查

先使用du命令查看哪个目录占用空间比较大
```
#du -h --max-depth=1 /var/lib/kubelet/pods |sort -hr| head -10
321G    /var/lib/kubelet/pods/2320b43f-fb7c-43ea-9cf8-a34b9aefec82
424K    /var/lib/kubelet/pods/29c89721-b641-4ada-a463-3b63ceca3836
```

然后通过如下命令找到具体的服务:

```
[root@iZt4nci8m38ekzb6r7th29Z pods]# kubectl get pods -A -o custom-columns=NodeName:.spec.nodeName,PodName:.metadata.name,PodUID:.metadata.uid | grep 2320b43f-fb7c-43ea-9cf8-a34b9aefec82
k8s-work-01      app-0                                2320b43f-fb7c-43ea-9cf8-a34b9aefec82

```

最后进入exec进入具体的pod查看原因, 然后解决就行:

```

(pyenv3) ➜  ~ kubectl exec -it app-7db59bc5b6 -c app-api  /bin/sh -n ops

-it后面紧跟的是pod的名称
-c后面是容器的名字, 因为一个pod里面可以同时拥有对个容器, 默认会进入第一个

可以通过kubectl describe pod xxx -n ops 这个命令获取所有pod中容器的名称, 然后指定即可.
```



